分词器初始化完成。词汇表大小: 10004
加载并拼接完成，文本长度：10071999字符
开始对全部文本进行编码...
Traceback (most recent call last):
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\model\train.py", line 248, in <module>
    train_process(config)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\model\train.py", line 146, in train_process
    all_token_ids = tokenizer.encode_corpus(raw_text_data)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 193, in encode_corpus
    subwords = self._split_word_to_subwords(word)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 65, in _split_word_to_subwords
    for length in [max_len] + [l for l in self.sorted_lengths if l < max_len and l <= n - start]:
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 65, in <listcomp>
    for length in [max_len] + [l for l in self.sorted_lengths if l < max_len and l <= n - start]:
KeyboardInterrupt
Traceback (most recent call last):
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\model\train.py", line 248, in <module>
    train_process(config)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\model\train.py", line 146, in train_process
    all_token_ids = tokenizer.encode_corpus(raw_text_data)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 193, in encode_corpus
    subwords = self._split_word_to_subwords(word)
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 65, in _split_word_to_subwords
    for length in [max_len] + [l for l in self.sorted_lengths if l < max_len and l <= n - start]:
  File "C:\Users\Ruizhe\Desktop\Study\Code\AI\transformer\tokenizer\Tokenizer.py", line 65, in <listcomp>
    for length in [max_len] + [l for l in self.sorted_lengths if l < max_len and l <= n - start]:
KeyboardInterrupt
