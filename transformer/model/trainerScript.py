from littleTransformer import DecoderOnlyTransformer
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, IterableDataset
import tqdm
import json
import os
from datetime import datetime, timedelta
import sys
import wandb
from huggingface_hub import HfApi, login
import time

torch.autograd.set_detect_anomaly(True)

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from tokenizer.Tokenizer import BPETokenizer

# ------------------------------
# æŠ˜ä¸­æ–¹æ¡ˆï¼šéƒ¨åˆ†é¢„è®¡ç®—ä¼°ç®—æ€»tokenæ•°
# ------------------------------
def estimate_total_tokens(file_path, tokenizer, sample_size=1000, max_length=100_000_000):
    """åªè®¡ç®—å‰Nä¸ªæ ·æœ¬çš„tokenæ•°ï¼Œä¼°ç®—æ€»é‡ï¼ˆå¤§å¹…å‡å°‘é¢„è®¡ç®—æ—¶é—´ï¼‰"""
    total_samples = 0  # æ€»æ ·æœ¬æ•°
    sample_tokens = 0  # æŠ½æ ·æ ·æœ¬çš„æ€»tokenæ•°
    current_length = 0
    
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    items = list(data.items())  # è½¬ä¸ºåˆ—è¡¨æ–¹ä¾¿æŠ½æ ·
    
    # 1. å…ˆç»Ÿè®¡æ€»æ ·æœ¬æ•°å’ŒæŠ½æ ·è®¡ç®—å¹³å‡tokenæ•°
    for i, (key, value) in enumerate(items):
        record_text = " ".join([
            f"QUESTION: {value['QUESTION']}",
            f"CONTEXT: {' '.join(value['CONTEXTS'])}",
            f"LONG_ANSWER: {value['LONG_ANSWER']}",
            "<EOS> <SOS>"
        ])
        
        # æ§åˆ¶æ–‡æœ¬æ€»é•¿åº¦ä¸Šé™
        if current_length + len(record_text) > max_length:
            break
        current_length += len(record_text)
        total_samples += 1
        
        # åªå¯¹å‰sample_sizeä¸ªæ ·æœ¬è®¡ç®—tokenæ•°ï¼ˆä¼°ç®—å¹³å‡ï¼‰
        if i < sample_size:
            tokens = tokenizer.encode(record_text, max_len=100000)
            sample_tokens += len(tokens)
    
    # 2. ä¼°ç®—æ€»tokenæ•° = æ€»æ ·æœ¬æ•° Ã— å¹³å‡æ¯ä¸ªæ ·æœ¬çš„tokenæ•°
    if total_samples == 0:
        return 0
    avg_tokens_per_sample = sample_tokens / min(sample_size, total_samples)  # é¿å…é™¤ä»¥0
    estimated_total = int(total_samples * avg_tokens_per_sample)
    print(f"åŸºäº{min(sample_size, total_samples)}ä¸ªæ ·æœ¬ä¼°ç®—ï¼šæ€»æ ·æœ¬æ•°={total_samples}ï¼Œå¹³å‡tokenæ•°={avg_tokens_per_sample:.1f}")
    return estimated_total

# ------------------------------
# 1. æ•°æ®ç”Ÿæˆå™¨å’Œæ•°æ®é›†ï¼ˆæ”¯æŒåŠ¨æ€åœæ­¢ï¼‰
# ------------------------------
def text_generator(file_path, max_length=100_000_000, stop_signal=None):
    """æ”¯æŒåŠ¨æ€åœæ­¢çš„æ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆç”¨äºæ§åˆ¶æ¯ä¸ªepochçš„æ•°æ®é‡ï¼‰"""
    current_length = 0
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    for key, value in data.items():
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰åœæ­¢ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
        if stop_signal and stop_signal['should_stop']:
            stop_signal['should_stop'] = False  # é‡ç½®ä¿¡å·
            break
            
        record_text = " ".join([
            f"QUESTION: {value['QUESTION']}",
            f"CONTEXT: {' '.join(value['CONTEXTS'])}",
            f"LONG_ANSWER: {value['LONG_ANSWER']}",
            "<EOS> <SOS>"
        ])
        
        if current_length + len(record_text) > max_length:
            break
        current_length += len(record_text)
        yield record_text

def token_generator(text_gen, tokenizer, buffer_size=10000, max_token_len=100000):
    token_buffer = []
    for text in text_gen:
        tokens = tokenizer.encode(text, max_len=max_token_len)
        token_buffer.extend(tokens)
        
        while len(token_buffer) >= buffer_size:
            yield token_buffer[:buffer_size]
            token_buffer = token_buffer[buffer_size:]
    
    if token_buffer:
        yield token_buffer

class StreamingTextDataset(IterableDataset):
    def __init__(self, token_gen, seq_len=100):
        self.token_gen = token_gen
        self.seq_len = seq_len
        self.token_buffer = []

    def __iter__(self):
        for token_batch in self.token_gen:
            self.token_buffer.extend(token_batch)
            
            while len(self.token_buffer) >= self.seq_len:
                yield torch.tensor(self.token_buffer[:self.seq_len], dtype=torch.long)
                self.token_buffer = self.token_buffer[1:]

# ------------------------------
# 2. è®­ç»ƒå™¨ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´æ‰¹æ¬¡ï¼‰
# ------------------------------
class Trainer:
    def __init__(self, model, optimizer, criterion, device, config, wandb_run=None):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.config = config
        self.save_path = config['save_path']
        self.best_val_loss = float('inf')
        self.wandb_run = wandb_run
        self.last_save_time = time.time()
        self.actual_train_tokens = 0  # è®°å½•å®é™…å¤„ç†çš„tokenæ•°ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
        
        self.hf_api = HfApi() if config['upload_to_hub'] else None
        if config['upload_to_hub'] and config['hub_token']:
            login(config['hub_token'], add_to_git_credential=True)

    def _upload_to_hub(self, file_path):
        try:
            self.hf_api.upload_file(
                path_or_fileobj=file_path,
                path_in_repo=os.path.basename(file_path),
                repo_id=self.config['hub_model_id'],
                repo_type="model"
            )
            print(f"âœ… å·²ä¸Šä¼  {file_path} åˆ° Hugging Face Hub")
        except Exception as e:
            print(f"âŒ Hugging Face ä¸Šä¼ å¤±è´¥: {str(e)}")

    def _log_and_upload(self, epoch, train_loss, val_loss):
        if self.wandb_run:
            self.wandb_run.log({
                "epoch": epoch,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "best_val_loss": self.best_val_loss,
                "actual_train_tokens": self.actual_train_tokens  # è®°å½•å®é™…å¤„ç†çš„tokenæ•°
            })
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "best_val_loss": self.best_val_loss,
            "actual_train_tokens": self.actual_train_tokens
        }
        with open("training_metrics.jsonl", "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry) + "\n")
        
        if self.config['upload_to_hub']:
            self._upload_to_hub("training_metrics.jsonl")

    def _save_model(self, epoch, is_best=False):
        os.makedirs("Models", exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

        if not is_best:
            # åˆ›å»ºæ¯ä¸ª epoch çš„å­ç›®å½•
            epoch_dir = f"Models/model_epoch_{epoch}_{timestamp}"
            os.makedirs(epoch_dir, exist_ok=True)
            save_name = f"{epoch_dir}/model_epoch_{epoch}_{timestamp}.pt"
        else:
            save_name = self.save_path  # æœ€ä½³æ¨¡å‹ä¿å­˜åˆ°å›ºå®šè·¯å¾„

        save_dict = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }
        if is_best:
            save_dict['loss'] = self.best_val_loss

        torch.save(save_dict, save_name)

        if self.config.get('upload_to_hub', False):
            try:
                self._upload_to_hub(save_name)
            except Exception as e:
                print(f"ä¸Šä¼ åˆ° Hub å¤±è´¥: {e}")

        return save_name

    def _train_epoch(self, train_loader, max_batches):
        self.model.train()
        self.actual_train_tokens = 0  # é‡ç½®è®¡æ•°
        epoch_loss, batch_count = 0, 0
        
        with tqdm.tqdm(total=max_batches, desc=f"è®­ç»ƒä¸­") as pbar:
            for batch in train_loader:
                if batch_count >= max_batches:
                    break
                
                # ç´¯åŠ å®é™…å¤„ç†çš„tokenæ•°ï¼ˆç”¨äºåç»­è°ƒæ•´ï¼‰
                self.actual_train_tokens += batch.numel()
                
                src = batch.to(self.device)
                inputs, targets = src[:, :-1], src[:, 1:]
                
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(
                    outputs.reshape(-1, outputs.shape[-1]),
                    targets.reshape(-1)
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
                pbar.update(1)
                pbar.set_postfix({"batch_loss": f"{loss.item():.4f}"})
        
        return epoch_loss / batch_count if batch_count > 0 else 0

    def _evaluate(self, val_loader, max_batches):
        self.model.eval()
        val_loss, batch_count = 0, 0
        
        with tqdm.tqdm(total=max_batches, desc=f"éªŒè¯ä¸­") as pbar:
            with torch.no_grad():
                for batch in val_loader:
                    if batch_count >= max_batches:
                        break
                    
                    src = batch.to(self.device)
                    inputs, targets = src[:, :-1], src[:, 1:]
                    
                    outputs = self.model(inputs)
                    loss = self.criterion(
                        outputs.reshape(-1, outputs.shape[-1]),
                        targets.reshape(-1)
                    )
                    val_loss += loss.item()
                    batch_count += 1
                    pbar.update(1)
                    pbar.set_postfix({"val_batch_loss": f"{loss.item():.4f}"})
        
        return val_loss / batch_count if batch_count > 0 else 0

    def train(self, num_epochs, train_loader, val_loader, train_max_batches, val_max_batches):
        for epoch in range(num_epochs):
            print(f"\n===== Epoch {epoch+1}/{num_epochs} =====")
            
            # åŠ¨æ€ä½¿ç”¨å½“å‰çš„æ‰¹æ¬¡ä¸Šé™
            train_loss = self._train_epoch(train_loader, train_max_batches)
            val_loss = self._evaluate(val_loader, val_max_batches)
            
            self._log_and_upload(epoch+1, train_loss, val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                saved_path = self._save_model(epoch+1, is_best=True)
                print(f"ğŸ“Œ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {saved_path} (éªŒè¯æŸå¤±: {val_loss:.4f})")
            
            # å®šæ—¶ä¿å­˜
            current_time = time.time()
            if (epoch + 1) % self.config['save_interval_epochs'] == 0 or \
               current_time - self.last_save_time >= self.config['save_interval_seconds']:
                
                saved_path = self._save_model(epoch+1)
                print(f"â±ï¸ å®šæ—¶ä¿å­˜æ¨¡å‹è‡³ {saved_path} (Epoch {epoch+1})")
                self.last_save_time = current_time

# ------------------------------
# ä¸»è®­ç»ƒæµç¨‹ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆæ ¸å¿ƒï¼‰
# ------------------------------
def train_process(config):
    # åˆå§‹åŒ–W&B
    wandb_run = wandb.init(
        project=config['wandb_project'],
        name=config.get('wandb_run_name'),
        config=config
    ) if config['use_wandb'] else None

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if wandb_run:
        wandb_run.config.update({"device": str(device)})

    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = BPETokenizer(config['vocab_path'])
    config['vocab_size'] = tokenizer.vocab_size
    config['pad_idx'] = tokenizer.PAD_ID
    print(f"åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆã€‚è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}")

    # ------------------------------
    # æŠ˜ä¸­æ–¹æ¡ˆï¼šå¿«é€Ÿä¼°ç®—æ‰¹æ¬¡ï¼ˆæ ¸å¿ƒï¼‰
    # ------------------------------
    # 1. å¿«é€Ÿä¼°ç®—è®­ç»ƒé›†æ€»tokenæ•°ï¼ˆåªæŠ½æ ·å‰Nä¸ªæ ·æœ¬ï¼‰
    print("å¿«é€Ÿä¼°ç®—è®­ç»ƒé›†tokenæ•°ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰...")
    estimated_train_tokens = estimate_total_tokens(
        file_path=config['data_path'],
        tokenizer=tokenizer,
        sample_size=config['estimate_sample_size'],  # å¯è°ƒæ•´æŠ½æ ·é‡ï¼ˆé»˜è®¤1000ï¼‰
        max_length=config['context_length']
    )
    # è®¡ç®—åˆå§‹æ‰¹æ¬¡ä¸Šé™ï¼ˆæ¯”ä¼°ç®—å€¼ç•¥å¤§ï¼Œé¿å…æå‰ç»“æŸï¼‰
    initial_train_batches = max(1, estimated_train_tokens // config['max_len'] // config['batch_size'] * 1.2)
    initial_train_batches = int(initial_train_batches)
    print(f"ä¼°ç®—è®­ç»ƒé›†tokenæ•°: {estimated_train_tokens:,}ï¼Œåˆå§‹æ‰¹æ¬¡ä¸Šé™: {initial_train_batches}")

    # 2. åŒç†ä¼°ç®—éªŒè¯é›†
    print("å¿«é€Ÿä¼°ç®—éªŒè¯é›†tokenæ•°...")
    estimated_val_tokens = estimate_total_tokens(
        file_path=config['data_path'],
        tokenizer=tokenizer,
        sample_size=config['estimate_sample_size'] // 5,  # éªŒè¯é›†æŠ½æ ·æ›´å°‘
        max_length=config['context_length'] // 10
    )
    initial_val_batches = max(1, estimated_val_tokens // config['max_len'] // config['batch_size'] * 1.2)
    initial_val_batches = int(initial_val_batches)
    print(f"ä¼°ç®—éªŒè¯é›†tokenæ•°: {estimated_val_tokens:,}ï¼Œåˆå§‹æ‰¹æ¬¡ä¸Šé™: {initial_val_batches}")

    # 3. æµå¼æ•°æ®åŠ è½½ï¼ˆæ”¯æŒåŠ¨æ€åœæ­¢ï¼‰
    stop_signal = {'should_stop': False}  # ç”¨äºæ§åˆ¶ç”Ÿæˆå™¨åœæ­¢çš„ä¿¡å·
    
    train_text_gen = text_generator(
        file_path=config['data_path'],
        max_length=config['context_length'],
        stop_signal=stop_signal
    )
    train_token_gen = token_generator(
        train_text_gen, 
        tokenizer, 
        buffer_size=50000,
        max_token_len=config['max_len'] * 10
    )

    val_text_gen = text_generator(
        file_path=config['data_path'],
        max_length=config['context_length'] // 10,
        stop_signal=stop_signal
    )
    val_token_gen = token_generator(
        val_text_gen, 
        tokenizer, 
        buffer_size=10000,
        max_token_len=config['max_len'] * 10
    )

    # æ•°æ®é›†å’ŒåŠ è½½å™¨
    train_dataset = StreamingTextDataset(train_token_gen, seq_len=config['max_len'])
    val_dataset = StreamingTextDataset(val_token_gen, seq_len=config['max_len'])

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=0)
    print("æµå¼DataLoaderå·²å‡†å¤‡å¥½ï¼Œå¼€å§‹è®­ç»ƒ...")

    # æ¨¡å‹åˆå§‹åŒ–
    model = DecoderOnlyTransformer(
        vocab_size=config['vocab_size'],
        d_model=config['d_model'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        d_ff=config['d_ff'],
        max_len=config['max_len'],
        dropout=config['dropout']
    ).to(device)
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # è®­ç»ƒåˆå§‹åŒ–
    criterion = nn.CrossEntropyLoss(ignore_index=config['pad_idx'])
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])
    trainer = Trainer(
        model=model,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        config=config,
        wandb_run=wandb_run
    )

    # å¯åŠ¨è®­ç»ƒï¼ˆä½¿ç”¨åˆå§‹æ‰¹æ¬¡ä¸Šé™ï¼‰
    trainer.train(
        num_epochs=config['num_epochs'],
        train_loader=train_loader,
        val_loader=val_loader,
        train_max_batches=initial_train_batches,
        val_max_batches=initial_val_batches
    )
    print("\n--- è®­ç»ƒå®Œæˆ ---")

    if wandb_run:
        wandb_run.finish()

if __name__ == "__main__":
    config = {
        # åŸºç¡€é…ç½®
        'vocab_path': '../tokenizer/vocab_test_set.npy',
        'data_path': r'C:\Users\Ruizhe\Desktop\Study\ID2221\Project\Data\pubmedqa\ori_pqau.json',
        'save_path': 'best_model.pt',
        'context_length': 1000_000_000,
        'max_len': 100,
        
        # æŠ˜ä¸­æ–¹æ¡ˆå‚æ•°ï¼ˆæ ¸å¿ƒï¼‰
        'estimate_sample_size': 1000,  # æŠ½æ ·å¤šå°‘ä¸ªæ ·æœ¬ä¼°ç®—tokenæ•°ï¼ˆå¯è°ƒï¼‰
        
        # æ¨¡å‹å‚æ•°
        'd_model': 768,
        'num_heads': 8,
        'num_layers': 6,
        'd_ff': 3072,
        'dropout': 0.1,
        
        # è®­ç»ƒå‚æ•°
        'batch_size': 32,
        'learning_rate': 5e-5,
        'num_epochs': 2,
        
        # ä¿å­˜é…ç½®
        'save_interval_epochs': 1,
        'save_interval_seconds': 21600,
        
        # W&Bå’ŒHFé…ç½®
        'use_wandb': True,
        'wandb_project': 'pubmedqa-training',
        'wandb_run_name': f"balanced-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        'upload_to_hub': False,
        'hub_model_id': 'your_username/pubmedqa-model',
        'hub_token': 'your_hf_token'
    }
    
    train_process(config)
